"""
llm_client.py
LLM abstraction layer.

Supports:
  - OpenAI GPT-4o / GPT-3.5-turbo
  - Stub mode (rule-based responses) when no API key is present
  - Hindi language response toggle
  - Retry logic with exponential backoff
  - Safe error fallbacks
"""

import os
import time
import logging
from typing import List, Dict, Optional

logger = logging.getLogger(__name__)

# Try importing openai
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    logger.warning("openai library not installed. Running in stub mode.")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
DEFAULT_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
MAX_RETRIES = 3
RETRY_DELAY = 1.5   # seconds


# ---------------------------------------------------------------------------
# Hindi system prompt instruction
# ---------------------------------------------------------------------------

HINDI_INSTRUCTION = (
    "Please respond entirely in Hindi (Devanagari script). "
    "Use respectful and warm language."
)

SYSTEM_PROMPT_TEMPLATE = """You are Astro, a warm and wise AI astrologer specializing in Vedic astrology.
You provide personalized astrological guidance based on the user's birth chart, zodiac sign, and current planetary influences.

Your style:
- Warm, empathetic, and encouraging
- Specific to the user's zodiac sign and profile
- Grounded in the retrieved astrological context (if provided)
- Never make absolute predictions â€” speak in terms of tendencies and possibilities

User Profile:
{user_profile_summary}

{conversation_context}

{hindi_instruction}

When retrieved astrological context is provided, use it to ground your response.
When no context is provided (conversational turns), rely on your astrological knowledge and the conversation history.
Always personalize your response to the user's zodiac sign ({zodiac_sign}).
"""


class LLMClient:
    """
    Abstraction over OpenAI API.
    Falls back to rule-based stub responses when API key is absent.
    """

    def __init__(self):
        self._client = None
        if OPENAI_AVAILABLE and OPENAI_API_KEY:
            self._client = OpenAI(api_key=OPENAI_API_KEY)
            logger.info(f"OpenAI LLM client initialized with model: {DEFAULT_MODEL}")
        else:
            logger.info("LLM running in STUB mode (no OpenAI key detected).")

    def generate(
        self,
        user_message: str,
        system_prompt: str,
        chat_history: List[Dict],
        max_tokens: int = 512,
    ) -> str:
        """
        Generate a response from the LLM.

        Args:
            user_message: The latest user message
            system_prompt: Constructed system prompt with context
            chat_history: List of prior turns as OpenAI-format messages
            max_tokens: Token budget for response

        Returns:
            Response string
        """
        if self._client:
            return self._openai_generate(user_message, system_prompt, chat_history, max_tokens)
        else:
            return self._stub_generate(user_message, system_prompt)

    def _openai_generate(self, user_message, system_prompt, chat_history, max_tokens) -> str:
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(chat_history)
        messages.append({"role": "user", "content": user_message})

        for attempt in range(1, MAX_RETRIES + 1):
            try:
                response = self._client.chat.completions.create(
                    model=DEFAULT_MODEL,
                    messages=messages,
                    max_tokens=max_tokens,
                    temperature=0.7,
                )
                return response.choices[0].message.content.strip()
            except Exception as e:
                logger.warning(f"OpenAI attempt {attempt}/{MAX_RETRIES} failed: {e}")
                if attempt < MAX_RETRIES:
                    time.sleep(RETRY_DELAY * attempt)
                else:
                    logger.error("All OpenAI retries exhausted.")
                    return self._error_fallback(user_message)

    def _stub_generate(self, user_message: str, system_prompt: str) -> str:
        """
        Rule-based stub response when OpenAI is unavailable.
        Extracts context hints from the system prompt to produce semi-relevant responses.
        """
        msg_lower = user_message.lower()
        is_hindi = "Hindi" in system_prompt

        # Extract zodiac from system prompt
        zodiac = "your zodiac sign"
        for line in system_prompt.splitlines():
            if "Zodiac Sign:" in line:
                zodiac = line.split("Zodiac Sign:")[-1].split("(")[0].strip()
                break

        if any(w in msg_lower for w in ["career", "job", "work", "kaam", "naukri"]):
            if is_hindi:
                return (f"à¤†à¤ªà¤•à¥‡ à¤²à¤¿à¤ à¤¯à¤¹ à¤¸à¤®à¤¯ à¤•à¤°à¤¿à¤¯à¤° à¤®à¥‡à¤‚ à¤®à¥‡à¤¹à¤¨à¤¤ à¤”à¤° à¤à¤•à¤¾à¤—à¥à¤°à¤¤à¤¾ à¤•à¤¾ à¤¹à¥ˆà¥¤ "
                        f"{zodiac} à¤°à¤¾à¤¶à¤¿ à¤•à¥‡ à¤²à¤¿à¤ à¤—à¥à¤°à¥ à¤—à¥à¤°à¤¹ à¤•à¤¾ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤…à¤µà¤¸à¤° à¤²à¤¾ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤ "
                        f"à¤¨à¤ˆ à¤œà¤¿à¤®à¥à¤®à¥‡à¤¦à¤¾à¤°à¤¿à¤¯à¤¾à¤ à¤† à¤¸à¤•à¤¤à¥€ à¤¹à¥ˆà¤‚ â€” à¤‰à¤¨à¥à¤¹à¥‡à¤‚ à¤†à¤¤à¥à¤®à¤µà¤¿à¤¶à¥à¤µà¤¾à¤¸ à¤¸à¥‡ à¤¸à¥à¤µà¥€à¤•à¤¾à¤° à¤•à¤°à¥‡à¤‚à¥¤")
            return (f"As a {zodiac}, this is a powerful time for career growth. "
                    f"Jupiter's influence opens doors to new opportunities and recognition. "
                    f"Focus on teamwork and clear communication â€” success is within reach.")

        elif any(w in msg_lower for w in ["love", "relationship", "partner", "pyar", "rishta"]):
            if is_hindi:
                return (f"à¤†à¤ªà¤•à¥‡ à¤ªà¥à¤°à¥‡à¤® à¤œà¥€à¤µà¤¨ à¤®à¥‡à¤‚ à¤¶à¥à¤•à¥à¤° à¤—à¥à¤°à¤¹ à¤¸à¤•à¤¾à¤°à¤¾à¤¤à¥à¤®à¤• à¤Šà¤°à¥à¤œà¤¾ à¤²à¤¾ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤ "
                        f"{zodiac} à¤°à¤¾à¤¶à¤¿ à¤•à¥‡ à¤²à¤¿à¤ à¤­à¤¾à¤µà¤¨à¤¾à¤¤à¥à¤®à¤• à¤¸à¤‚à¤µà¤¾à¤¦ à¤”à¤° à¤¸à¤®à¤ à¤¬à¤¹à¥à¤¤ à¤®à¤¹à¤¤à¥à¤µà¤ªà¥‚à¤°à¥à¤£ à¤¹à¥ˆà¥¤ "
                        f"à¤–à¥à¤²à¤•à¤° à¤…à¤ªà¤¨à¥€ à¤­à¤¾à¤µà¤¨à¤¾à¤à¤‚ à¤µà¥à¤¯à¤•à¥à¤¤ à¤•à¤°à¥‡à¤‚à¥¤")
            return (f"For {zodiac}, Venus is currently enhancing your emotional connections. "
                    f"Open and honest communication will strengthen your relationship. "
                    f"This is a beautiful time for deepening bonds.")

        elif any(w in msg_lower for w in ["stress", "anxious", "worried", "pareshan", "chinta"]):
            if is_hindi:
                return (f"à¤†à¤œ à¤•à¤¾ à¤¤à¤¨à¤¾à¤µ à¤¶à¤¨à¤¿ à¤”à¤° à¤®à¤‚à¤—à¤² à¤•à¥‡ à¤ªà¥à¤°à¤­à¤¾à¤µ à¤¸à¥‡ à¤¹à¥‹ à¤¸à¤•à¤¤à¤¾ à¤¹à¥ˆà¥¤ "
                        f"à¤§à¥à¤¯à¤¾à¤¨ à¤”à¤° à¤—à¤¹à¤°à¥€ à¤¸à¤¾à¤‚à¤¸ à¤²à¥‡à¤¨à¥‡ à¤•à¤¾ à¤…à¤­à¥à¤¯à¤¾à¤¸ à¤•à¤°à¥‡à¤‚à¥¤ "
                        f"{zodiac} à¤°à¤¾à¤¶à¤¿ à¤•à¥‡ à¤²à¤¿à¤ à¤†à¤œ à¤•à¤¾ à¤¦à¤¿à¤¨ à¤¸à¤‚à¤¯à¤® à¤”à¤° à¤§à¥ˆà¤°à¥à¤¯ à¤•à¤¾ à¤¹à¥ˆà¥¤")
            return (f"The stress you're feeling today may be influenced by Saturn's current transit. "
                    f"As a {zodiac}, grounding practices like meditation can help significantly. "
                    f"Trust that this challenging phase will pass and bring growth.")

        elif any(w in msg_lower for w in ["spiritual", "meditation", "karma", "dhyan"]):
            if is_hindi:
                return (f"à¤†à¤ªà¤•à¥€ à¤†à¤§à¥à¤¯à¤¾à¤¤à¥à¤®à¤¿à¤• à¤¯à¤¾à¤¤à¥à¤°à¤¾ à¤®à¥‡à¤‚ à¤•à¥‡à¤¤à¥ à¤—à¥à¤°à¤¹ à¤®à¤¾à¤°à¥à¤—à¤¦à¤°à¥à¤¶à¤¨ à¤¦à¥‡ à¤°à¤¹à¤¾ à¤¹à¥ˆà¥¤ "
                        f"à¤§à¥à¤¯à¤¾à¤¨ à¤”à¤° à¤•à¥ƒà¤¤à¤œà¥à¤žà¤¤à¤¾ à¤¸à¥‡ à¤†à¤‚à¤¤à¤°à¤¿à¤• à¤¶à¤¾à¤‚à¤¤à¤¿ à¤®à¤¿à¤²à¥‡à¤—à¥€à¥¤ "
                        f"{zodiac} à¤°à¤¾à¤¶à¤¿ à¤•à¥‡ à¤²à¤¿à¤ à¤¯à¤¹ à¤†à¤¤à¥à¤®à¤šà¤¿à¤‚à¤¤à¤¨ à¤•à¤¾ à¤‰à¤¤à¥à¤•à¥ƒà¤·à¥à¤Ÿ à¤¸à¤®à¤¯ à¤¹à¥ˆà¥¤")
            return (f"Your spiritual path is deeply influenced by Ketu's energy right now. "
                    f"As a {zodiac}, this is an excellent time for inner reflection and meditation. "
                    f"The universe is guiding you toward greater wisdom.")

        elif any(w in msg_lower for w in ["summar", "told", "recap", "dobara"]):
            if is_hindi:
                return "à¤…à¤­à¥€ à¤¤à¤• à¤¹à¤®à¤¨à¥‡ à¤•à¤°à¤¿à¤¯à¤°, à¤ªà¥à¤°à¥‡à¤® à¤œà¥€à¤µà¤¨ à¤”à¤° à¤†à¤ªà¤•à¥€ à¤°à¤¾à¤¶à¤¿ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤¬à¤¾à¤¤ à¤•à¥€ à¤¹à¥ˆà¥¤ à¤•à¥à¤¯à¤¾ à¤†à¤ª à¤•à¤¿à¤¸à¥€ à¤µà¤¿à¤¶à¥‡à¤· à¤µà¤¿à¤·à¤¯ à¤ªà¤° à¤”à¤° à¤œà¤¾à¤¨à¤•à¤¾à¤°à¥€ à¤šà¤¾à¤¹à¤¤à¥‡ à¤¹à¥ˆà¤‚?"
            return (f"So far in our conversation, we've discussed your astrological profile as a {zodiac}. "
                    f"I've shared insights about planetary influences affecting your life. "
                    f"Is there a specific area you'd like to explore further?")

        else:
            if is_hindi:
                return (f"à¤¨à¤®à¤¸à¥à¤¤à¥‡! à¤®à¥ˆà¤‚ Astro à¤¹à¥‚à¤‚, à¤†à¤ªà¤•à¤¾ à¤µà¥à¤¯à¤•à¥à¤¤à¤¿à¤—à¤¤ à¤œà¥à¤¯à¥‹à¤¤à¤¿à¤· à¤¸à¤¹à¤¾à¤¯à¤•à¥¤ "
                        f"{zodiac} à¤°à¤¾à¤¶à¤¿ à¤•à¥‡ à¤°à¥‚à¤ª à¤®à¥‡à¤‚, à¤†à¤ªà¤•à¥‡ à¤œà¥€à¤µà¤¨ à¤®à¥‡à¤‚ à¤—à¥à¤°à¤¹à¥‹à¤‚ à¤•à¤¾ à¤µà¤¿à¤¶à¥‡à¤· à¤ªà¥à¤°à¤­à¤¾à¤µ à¤¹à¥ˆà¥¤ "
                        f"à¤†à¤ª à¤•à¤°à¤¿à¤¯à¤°, à¤ªà¥à¤°à¥‡à¤®, à¤¯à¤¾ à¤†à¤§à¥à¤¯à¤¾à¤¤à¥à¤®à¤¿à¤•à¤¤à¤¾ à¤•à¥‡ à¤¬à¤¾à¤°à¥‡ à¤®à¥‡à¤‚ à¤ªà¥‚à¤› à¤¸à¤•à¤¤à¥‡ à¤¹à¥ˆà¤‚à¥¤")
            return (f"Namaste! As a {zodiac}, the stars have a fascinating story to tell about your journey. "
                    f"I'm here to guide you through astrological insights about career, love, spirituality, "
                    f"and planetary influences. What would you like to explore today?")

    def _error_fallback(self, user_message: str) -> str:
        return (
            "I apologize â€” I'm experiencing a temporary connection issue. "
            "Please try again in a moment. The stars are still watching over you! ðŸŒŸ"
        )


def build_system_prompt(
    user_profile_summary: str,
    zodiac_sign: str,
    conversation_context: str,
    retrieved_context: str,
    preferred_language: str = "en",
) -> str:
    """Construct the full system prompt for the LLM."""
    hindi_instruction = HINDI_INSTRUCTION if preferred_language == "hi" else ""

    if retrieved_context:
        rag_section = f"\nRetrieved Astrological Knowledge:\n{retrieved_context}\n"
    else:
        rag_section = ""

    prompt = SYSTEM_PROMPT_TEMPLATE.format(
        user_profile_summary=user_profile_summary,
        conversation_context=conversation_context,
        hindi_instruction=hindi_instruction,
        zodiac_sign=zodiac_sign,
    )
    return prompt + rag_section


# Singleton
_llm_client: LLMClient = None


def get_llm_client() -> LLMClient:
    global _llm_client
    if _llm_client is None:
        _llm_client = LLMClient()
    return _llm_client
